# CPM

CPM is an open-source program on large-scale pre-trained models, which is conducted by Beijing Academy of Artificial Intelligence and Tsinghua University, with the goal of building large-scale Chinese-centered pre-trained models. The open-source models can be widely used in Chinese natural language understanding, generative tasks, and all of them are free and open for download for research use.

**If you want better efficiency during inference, we recommend [BMInf](https://github.com/OpenBMB/BMInf), which supports the inference of big models with a single GTX 1060 or later ones.**

## CPM-1

CPM: A Large-scale Generative **C**hinese **P**re-trained Language **M**odel. [[paper](https://arxiv.org/abs/2012.00413)]

Codes:
* [CPM-1-Pretrain](https://github.com/TsinghuaAI/CPM-1-Pretrain)
* [CPM-1-Finetune](https://github.com/TsinghuaAI/CPM-1-Finetune)
* [CPM-1-Generate](https://github.com/TsinghuaAI/CPM-1-Generate)
* [CPM-1-Distill](https://github.com/TsinghuaAI/CPM-1-Distill)
* [TDS](https://github.com/TsinghuaAI/TDS)

Models: [[Download](https://cpm.baai.ac.cn/login.html?path=%2Fdownload.html)]

## CPM-2

CPM-2: Large-scale **C**ost-effective **P**re-trained Language **M**odels. [[paper](https://arxiv.org/pdf/2106.10715)]

Codes:
* [CPM-2-Pretrain](https://github.com/TsinghuaAI/CPM-2-Pretrain)
* [CPM-2-Finetune](https://github.com/TsinghuaAI/CPM-2-Finetune)
* [InfMoE](https://github.com/TsinghuaAI/InfMoE)

Models: [[Download](https://resource.wudaoai.cn/home?ind=2)]

## PLM Survey

Pre-Trained Models: Past, Present and Future. [[paper](https://arxiv.org/abs/2106.07139)]

## Useful Links

* Article Generation with CPM-1. [[link](https://github.com/yangjianxin1/CPM)]

## Cite

```
@article{cpm-v1,
  title={CPM: A Large-scale Generative Chinese Pre-trained Language Model},
  author={Zhang, Zhengyan and Han, Xu, and Zhou, Hao, and Ke, Pei, and Gu, Yuxian and Ye, Deming and Qin, Yujia and Su, Yusheng and Ji, Haozhe and Guan, Jian and Qi, Fanchao and Wang, Xiaozhi and Zheng, Yanan and Zeng, Guoyang and Cao, Huanqi and Chen, Shengqi and Li, Daixuan and Sun, Zhenbo and Liu, Zhiyuan and Huang, Minlie and Han, Wentao and Tang, Jie and Li, Juanzi and Sun, Maosong},
  year={2020}
}

@article{cpm-v2,
  title={CPM-2: Large-scale Cost-efficient Pre-trained Language Models},
  author={Zhang, Zhengyan and Gu, Yuxian and Han, Xu and Chen, Shengqi and Xiao, Chaojun and Sun, Zhenbo and Yao, Yuan and Qi, Fanchao and Guan, Jian and Ke, Pei and Cai, Yanzheng and Zeng, Guoyang and Tan, Zhixing and Liu, Zhiyuan and Huang, Minlie and Han, Wentao and Liu, Yang and Zhu, Xiaoyan and Sun, Maosong},
  year={2021}
}

@article{han2021pretrained,
      title={Pre-Trained Models: Past, Present and Future}, 
      author={Xu Han and Zhengyan Zhang and Ning Ding and Yuxian Gu and Xiao Liu and Yuqi Huo and Jiezhong Qiu and Liang Zhang and Wentao Han and Minlie Huang and Qin Jin and Yanyan Lan and Yang Liu and Zhiyuan Liu and Zhiwu Lu and Xipeng Qiu and Ruihua Song and Jie Tang and Ji-Rong Wen and Jinhui Yuan and Wayne Xin Zhao and Jun Zhu},
      year={2021}
}

```
